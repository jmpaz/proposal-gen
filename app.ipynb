{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chain():\n",
    "    llm = OpenAI(model_name=\"text-chat-davinci-002-20230126\", temperature=0.8)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['history', 'input'],\n",
    "        output_parser=None,\n",
    "        template='You are Assistant, a large language model trained by OpenAI and designed to assist with a wide range of tasks, often providing valuable insights and information on a wide range of topics. When needed, messages should be enclosed in an appropriate number of backticks or double quotes, depending on the contents of the input or output message. Please make sure to properly style your responses using Github Flavored Markdown. Use markdown syntax for things like headings, lists, tables, quotes, colored text, code blocks, highlights, superscripts, etc, etc. For emojis, use unicode. Make sure not to mention markdown or styling in your actual response.\\n\\nCurrent conversation:\\n{history}\\n\\nUser: \"\"\"\"\"\\n{input}\"\"\"\"\"\\n\\nAssistant: ',\n",
    "        template_format='f-string'\n",
    "    )\n",
    "\n",
    "    chain = ConversationChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        memory=ConversationBufferMemory(human_prefix=\"User\", ai_prefix=\"Assistant\")\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "chain = load_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load initializing prompt from file\n",
    "with open(\"prompt.txt\", \"r\") as f:\n",
    "    init_prompt = f.read()\n",
    "\n",
    "# run the chain with the initializing prompt\n",
    "output = chain.predict(input=init_prompt).strip(\"<|im_end|>\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.predict(input=\"What is your name and function?\").strip(\"<|im_end|>\") # run the chain, stripping \"<|im_end|>\" (appended to text-chat-davinci completions) from the end of the output string\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.predict(input=\"What can you do?\").strip(\"<|im_end|>\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
